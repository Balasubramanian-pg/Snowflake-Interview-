# Data Loading & Ingestion

## Questions

1. What is a Snowflake Stage and what are the different types available (Internal vs. External)?

2. Explain the COPY INTO command. What options and transformations does it support?

3. What file formats does Snowflake support for data loading, and which format is most efficient?

4. What is Snowpipe and how does it enable continuous, automated data ingestion?

5. How does Snowpipe's event-driven loading work with cloud storage (S3, GCS, Azure Blob)?

6. What is the difference between bulk loading and continuous loading in Snowflake?

7. How do you handle semi-structured data (JSON, Avro, Parquet, ORC) during ingestion?

8. What are file format objects in Snowflake and why should you use them instead of inline options?

9. How do you load data from an external stage without copying it into Snowflake storage?

10. Describe how you would set up an end-to-end pipeline using Snowflake Tasks and Streams for incremental loading.

11. What strategies do you use to maximize throughput when loading large files into Snowflake?

12. How do you monitor load jobs and troubleshoot failed COPY INTO operations?

---
[Back to README](README.md)
