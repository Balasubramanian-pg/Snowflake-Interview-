# How to enable applications deployed across multiple clusters to reliably share and access persistent data.

> [!Information]
> Multi-cluster shared data architectures provide mechanisms for applications distributed across distinct clusters to access a common, consistent, and highly available data store.

## Why This Matters
- **Decision support**: Guides the selection of appropriate data storage technologies and replication strategies for distributed applications, influencing architectural choices for resilience, performance, and cost.
- **What breaks or costs time if misunderstood**: Leads to data inconsistencies, increased network latency, complex operational overhead, potential data loss during failovers, and inability to scale applications globally or across regions effectively.
- **When you would look this up in real life**: When designing applications for disaster recovery, multi-region deployments, hybrid cloud strategies, or when migrating monolithic applications with shared databases to a distributed microservices architecture.

## Key Points
- **Data Locality vs. Global Access**: Balancing the need for data to be physically close to consuming applications for low latency against the requirement for global accessibility and consistency across all clusters.
- **Consistency Models**: Different architectures support varying consistency guarantees (e.g., strong, eventual, causal), which impact data freshness, application complexity, and performance trade-offs.
- **Network Latency Impact**: The physical distance and network path between clusters significantly affect data synchronization speed, read/write performance, and the feasibility of synchronous replication.
- **Replication Strategies**: Data can be replicated synchronously (high consistency, higher latency) or asynchronously (lower latency, eventual consistency) using various topologies like active-passive, active-active, or sharding.
- **Shared Storage Technologies**: Common solutions include distributed file systems, global object storage, globally distributed databases, and message queues acting as data conduits.
- **Security and Access Control**: Implementing robust authentication, authorization, and encryption mechanisms that span multiple clusters and potentially different cloud providers is critical for data integrity and compliance.

> [!important]
> Data integrity and availability must be maintained across all clusters, often requiring careful consideration of the CAP theorem and explicit trade-offs between consistency, availability, and partition tolerance.

## Mental Model
- **Input**: An application instance in `Cluster A` needs to read or write data that is also accessible and potentially modified by an application instance in `Cluster B`.
- **Transformation**: A shared data layer, such as a globally distributed database or object storage service, acts as the central repository. Data changes originating from `Cluster A` are propagated to this shared layer, which then ensures consistency and availability for `Cluster B` (and vice-versa) according to the configured replication and consistency model.
- **Output**: Both `Cluster A` and `Cluster B` applications can reliably access and modify the shared data, with changes visible across clusters within the bounds of the chosen consistency model.

## Gotchas
- **Network Egress Costs**: Transferring large volumes of data between clusters, especially across different cloud regions or providers, can incur significant and often unexpected network egress charges.
- **Split-Brain Scenarios**: In active-active replication setups, network partitions can lead to independent updates on different clusters, resulting in conflicting data that is difficult to reconcile without careful design.
- **Operational Complexity**: Managing and monitoring distributed data stores across multiple clusters introduces significant operational overhead, requiring specialized tooling and expertise for deployment, scaling, backup, and recovery.
- **Vendor Lock-in**: Relying heavily on specific cloud provider services for global data distribution can create strong vendor lock-in, making future migrations or multi-cloud strategies more challenging.
- **Data Gravity**: Once data is stored in a specific location, it can be expensive and complex to move, influencing where compute resources must be deployed to minimize latency.

> [!Tip]
> Consider a global e-commerce platform where user profiles and product catalogs must be consistent across multiple regional deployments (e.g., US, EU, APAC). A globally distributed database (like a multi-region relational database or a global NoSQL store) would replicate data asynchronously between regions, allowing users in any region to see the same product information and update their profiles with eventual consistency, while local caches handle read performance.