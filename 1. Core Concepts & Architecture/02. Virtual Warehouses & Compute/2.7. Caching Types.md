# Understanding Different Caching Types

> [!Information]
> Caching involves storing copies of data closer to the consumer to reduce latency and improve performance, with various types addressing different architectural layers and use cases.

## Why This Matters
- Choosing the appropriate caching strategy is critical for designing scalable, high-performance systems, balancing data freshness, cost, and complexity.
- Misunderstanding caching types can lead to slow response times, excessive infrastructure costs, stale data being served, or even system instability under load.
- You would look this up when architecting new systems, optimizing existing application performance, troubleshooting latency issues, or evaluating cloud service offerings for data storage and delivery.

## Key Points
- **Client-Side Caching**: Data stored directly on the user's device (e.g., browser cache, application-specific local storage). Reduces network round-trips to the server, improving perceived performance and reducing server load for repeat access.
- **CDN (Content Delivery Network) Caching**: Static and dynamic content cached at geographically distributed edge servers. Minimizes latency for users by serving content from a location physically closer to them, offloading traffic from origin servers.
- **Proxy Caching**: Intermediate servers (forward or reverse proxies) store responses from origin servers. Forward proxies cache for multiple clients, while reverse proxies cache for multiple origin servers, reducing load and improving response times for shared resources.
- **Application-Level Caching**: Data cached within the application's memory space or in a dedicated in-memory store (e.g., Redis, Memcached). Stores results of expensive computations, frequently accessed database queries, or session data to avoid re-computation or repeated database access.
- **Database Caching**: Specialized layers or features within a database system that cache query results, data blocks, or frequently accessed objects. Reduces the need for repetitive disk I/O and complex query execution, speeding up data retrieval.
- **Write-Through vs. Write-Back Caching**:
    - **Write-Through**: Data is written simultaneously to both the cache and the underlying persistent store. Ensures data consistency but can introduce higher write latency.
    - **Write-Back**: Data is written only to the cache initially, and then asynchronously written to the persistent store later. Offers lower write latency and higher throughput but risks data loss if the cache fails before data is persisted.

> [!important]
> Always prioritize data consistency and freshness requirements when selecting and implementing a caching strategy; the performance benefits of caching must not compromise data integrity.

## Mental Model
- **Step 1: Request Initiation** → A user or system requests data.
- **Step 2: Cache Check (Layered)** → The request first checks the nearest cache layer (e.g., browser cache, then CDN, then application cache).
- **Step 3: Cache Hit** → If the data is found and valid in a cache layer, it's served immediately from that cache, bypassing subsequent layers and the origin.
- **Step 4: Cache Miss** → If the data is not found or is invalid in a cache layer, the request proceeds to the next cache layer or eventually to the origin data source (e.g., database, API).
- **Step 5: Origin Fetch & Populate** → The origin data source retrieves or computes the data. This data is then returned to the requesting cache layer, which stores a copy (populates the cache) before forwarding it to the requester.
- **Step 6: Data Invalidation** → Mechanisms (e.g., Time-To-Live, explicit invalidation) ensure cached data is removed or marked stale when the original data changes, preventing the serving of outdated information.

## Gotchas
- **Stale Data**: Serving outdated information due to improper cache invalidation strategies or overly aggressive Time-To-Live (TTL) settings, leading to incorrect user experiences or operational errors.
- **Cache Stampede (Thundering Herd)**: When a cached item expires, and many concurrent requests simultaneously miss the cache, all hitting the origin server at once, potentially overwhelming it.
- **Over-Caching/Under-Caching**: Caching data that changes too frequently or is rarely accessed (overhead without benefit), versus failing to cache highly accessed, static data (missed performance opportunities).
- **Consistency Models**: Assuming strong consistency across distributed caches when eventual consistency is the default, leading to data synchronization issues and unexpected behavior.
- **Cache Key Design**: Poorly designed cache keys can lead to cache fragmentation (storing many slightly different versions of the same data) or cache collisions (different data sharing the same key).

> [!Tip]
> Consider an e-commerce product page: Your browser caches static assets (images, CSS) (Client-Side). A CDN serves product images globally (CDN Caching). The application caches product details from the database (Application-Level), and the database itself caches frequently accessed product records (Database Caching). When a product's price changes, the application explicitly invalidates its cache entry to ensure users see the updated price, demonstrating layered caching and invalidation.