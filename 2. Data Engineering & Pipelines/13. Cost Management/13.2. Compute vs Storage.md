# 13.2. Compute vs Storage

Canonical documentation for [13.2. Compute vs Storage](2. Data Engineering & Pipelines/13. Cost Management/13.2. Compute vs Storage.md). This document defines concepts, terminology, and standard usage.

## Purpose
The distinction between compute and storage addresses the fundamental architectural challenge of resource allocation in digital systems. Historically, these resources were physically and logically tethered within a single machine. In modern distributed systems, the separation of these two concerns allows for independent scaling, improved cost efficiency, and higher resiliency. This topic explores the trade-offs between integrated and decoupled architectures.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural principles rather than specific cloud provider services.

## Scope
This documentation covers the theoretical and practical boundaries of processing power versus data persistence.

> [!IMPORTANT]
> **In scope:**
> * Core functionality of processing (Compute) and persistence (Storage).
> * Theoretical boundaries of decoupled vs. coupled architectures.
> * Economic and performance trade-offs of resource scaling.
> * Data locality and movement principles.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., AWS EC2 vs. S3, Snowflake vs. BigQuery).
> * Physical hardware specifications (e.g., specific CPU architectures or NVMe protocols).
> * Networking protocols (TCP/IP, InfiniBand) except where they impact the compute-storage boundary.

## Definitions
| Term | Definition |
|------|------------|
| Compute | The CPU and RAM resources required to perform logic, transformations, and calculations on data. |
| Storage | The persistent medium (disk, flash, or object store) where data resides when not being actively processed. |
| Decoupling | The architectural practice of separating compute resources from storage resources so they can scale independently. |
| Data Gravity | The concept that data and applications are drawn together; as data sets grow, it becomes harder and more expensive to move them to compute. |
| Latency | The time delay between a compute request for data and the storage system's delivery of that data. |
| Throughput | The volume of data that can be moved between storage and compute within a given timeframe. |

## Core Concepts
The relationship between compute and storage is defined by how they interact across a network or bus.

**The Resource Relationship**
Compute is ephemeral and active; storage is persistent and passive. In a traditional model, compute and storage are "coupled" (e.g., a hard drive inside a laptop). In a modern "decoupled" model, compute nodes access a shared storage pool over a high-speed network.

> [!TIP]
> Think of a restaurant: The **Compute** is the chef (processing power), and the **Storage** is the pantry (data). In a coupled system, every chef has their own private, small pantry. In a decoupled system, many chefs share one massive, centralized walk-in freezer. The decoupled model allows you to hire more chefs during the dinner rush without needing to build more freezers.

**Scaling Dimensions**
*   **Vertical Scaling:** Increasing the power of a single unit (more CPU or more Disk on one machine).
*   **Horizontal Scaling:** Adding more units (more compute nodes or more storage clusters).

## Standard Model
The modern standard model for large-scale systems is the **Decoupled Architecture**.

1.  **Independent Scaling:** Compute can be scaled up or down based on the complexity of the workload (e.g., a heavy analytical query) without paying for additional storage. Conversely, storage can grow as data accumulates without requiring additional CPU.
2.  **Stateless Compute:** Compute nodes are treated as "cattle, not pets." Since data is stored externally, any compute node can fail and be replaced without data loss.
3.  **Centralized Truth:** A single storage layer acts as the "Source of Truth," preventing data silos that occur when data is trapped on local compute disks.

## Common Patterns
*   **Ephemeral Compute:** Spinning up a cluster of processing nodes to perform a specific task (like a batch job) and shutting them down immediately after, while the data remains in persistent storage.
*   **Tiered Storage:** Moving data between high-performance storage (close to compute) and low-cost "cold" storage based on access frequency.
*   **Caching (Local Storage):** Using a small amount of high-speed storage physically close to the compute (L1/L2 cache or local SSD) to reduce the latency inherent in decoupled systems.

## Anti-Patterns
*   **The "Monolithic" Scale:** Increasing storage capacity by buying more servers that include unnecessary CPU and RAM, leading to "stranded" or wasted resources.
*   **Data Exhaustion:** Designing a system where compute nodes store state locally without replication, leading to permanent data loss if the compute instance fails.
*   **Over-Provisioning:** Maintaining high-performance compute 24/7 for a storage-heavy workload that only requires occasional processing.

> [!CAUTION]
> Avoid tight coupling in distributed systems where the workload is unpredictable. Tight coupling forces you to scale for the "peak" of both resources simultaneously, which significantly increases operational costs.

## Edge Cases
*   **High-Performance Computing (HPC):** In scenarios requiring microsecond latency (e.g., high-frequency trading), the network delay of decoupled storage is unacceptable. Here, "Data Locality" (bringing compute to the storage) is preferred.
*   **Edge Computing:** In IoT or remote sensors, bandwidth to a central storage pool may be limited or non-existent, forcing a return to coupled architectures where processing must happen on-device.
*   **Heavy Data Shuffling:** If a compute task requires moving 100% of the data in storage across the network for every operation, the network becomes the bottleneck, neutralizing the benefits of decoupling.

## Related Topics
*   **13.1. Distributed Systems Theory:** The underlying logic of how nodes communicate.
*   **14.4. Data Locality:** The strategy of moving compute to where data resides.
*   **Cloud Economics:** The financial impact of pay-as-you-go compute vs. long-term storage.
*   **Stateless vs. Stateful Design:** How applications manage memory and persistence.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-17 | Initial AI-generated canonical documentation |