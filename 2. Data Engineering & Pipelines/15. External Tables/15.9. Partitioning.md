# 15.9. Partitioning

Canonical documentation for [15.9. Partitioning](2. Data Engineering & Pipelines/15. External Tables/15.9. Partitioning.md). This document defines concepts, terminology, and standard usage.

## Purpose
Partitioning is a structural design pattern used to decompose a large dataset or system into smaller, more manageable segments. The primary objective is to improve scalability, performance, and availability by reducing the contention for resources and limiting the scope of data operations. By isolating data subsets, partitioning allows systems to handle growth linearly and optimize resource utilization based on specific access patterns.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural principles of data and system segmentation.

## Scope
This documentation covers the theoretical frameworks and logical applications of partitioning across various computational domains, including databases, distributed systems, and storage architectures.

> [!IMPORTANT]
> **In scope:**
> * Logical and physical partitioning strategies.
> * Criteria for selecting partition keys.
> * Data distribution methodologies (Range, Hash, List).
> * Impact on system maintenance and query performance.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., PostgreSQL-specific syntax or AWS DynamoDB-specific limits).
> * Hardware-level disk partitioning (e.g., MBR vs. GPT).
> * Network-level micro-segmentation.

## Definitions
| Term | Definition |
|------|------------|
| Partition | A logical or physical subset of a larger dataset or system. |
| Partition Key | The attribute or set of attributes used to determine which partition a specific data element belongs to. |
| Horizontal Partitioning | Dividing a dataset by rows, where each partition contains a subset of the total records (often called Sharding). |
| Vertical Partitioning | Dividing a dataset by columns, where each partition contains a subset of the attributes for all records. |
| Skew | An uneven distribution of data across partitions, leading to resource imbalances. |
| Rebalancing | The process of redistributing data across partitions to correct skew or accommodate new capacity. |

## Core Concepts
The fundamental idea behind partitioning is "divide and conquer." By breaking a monolithic structure into autonomous units, a system can achieve higher throughput through parallel processing.

**The Partitioning Function**
At the heart of any partitioning strategy is a function $f(k) = p$, where $k$ is the partition key and $p$ is the resulting partition identifier. The goal of this function is to provide a deterministic mapping that ensures data can be consistently located.

> [!TIP]
> Think of partitioning like a library: instead of one giant pile of books, the library uses a "partitioning key" (the first letter of the author's last name) to place books on specific shelves. This allows multiple people to look for books simultaneously without bumping into each other.

## Standard Model
The standard model for partitioning involves three distinct layers:

1.  **The Routing Layer:** Receives a request, identifies the partition key, and determines the target partition.
2.  **The Distribution Logic:** The algorithm (Range, Hash, or List) that defines the boundaries of each partition.
3.  **The Storage Layer:** The physical medium or node where the partitioned data resides.

In a well-architected model, the routing layer is decoupled from the storage layer, allowing for partitions to be moved or split with minimal impact on the application logic.

## Common Patterns

### Range Partitioning
Data is assigned to partitions based on a continuous range of values. This is most common for time-series data or ordered sequences.
*   *Example:* Partitioning by `TransactionDate` where each month has its own partition.

### Hash Partitioning
A hash function is applied to the partition key to produce a value that determines the partition. This is ideal for ensuring an even distribution of data across a fixed number of partitions.
*   *Example:* `Hash(UserID) % NumberOfPartitions`.

### List Partitioning
Data is assigned to partitions based on a discrete list of values. This is used when the partition key has a limited set of categorical values.
*   *Example:* Partitioning by `Region` (e.g., 'North', 'South', 'East', 'West').

### Composite Partitioning
A combination of the above methods, such as Range-Hash partitioning, where data is first divided by range and then sub-partitioned by hash.

## Anti-Patterns

### Choosing a Low-Cardinality Key
Using a partition key with very few unique values (e.g., `Gender` or `BooleanStatus`) leads to massive partitions that cannot be further subdivided, defeating the purpose of partitioning.

### Hotspotting
Selecting a key that results in most operations hitting a single partition. For example, using a monotonically increasing timestamp as a hash key in a high-velocity write system can cause the "current" partition to become a bottleneck.

### Over-Partitioning
Creating too many partitions relative to the data volume. This introduces significant metadata overhead and can degrade performance due to the complexity of managing thousands of small files or connections.

> [!CAUTION]
> Avoid circular dependencies where the logic to find a partition depends on data stored within the partition itself. This creates a "chicken and egg" problem during system recovery.

## Edge Cases

### Partition Key Updates
Updating the value of a partition key is a complex operation. In most models, this requires deleting the record from the old partition and inserting it into the new one, which can break atomicity if not handled within a global transaction.

### Cross-Partition Joins
Queries that require data from multiple partitions are significantly more expensive than single-partition queries. They require the system to aggregate results from multiple sources, often leading to increased latency and network congestion.

### Empty Partitions
Systems must be designed to handle "ghost" partitions—partitions that exist in metadata but contain no data—to ensure that range scans or aggregate functions do not fail or return incorrect results.

## Related Topics
*   **15.10. Sharding:** The application of horizontal partitioning across multiple physical nodes.
*   **Data Locality:** The practice of moving computation close to the partition where the data resides.
*   **Consistency Models:** How partitioning affects CAP theorem trade-offs (Consistency, Availability, Partition Tolerance).

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-17 | Initial AI-generated canonical documentation |