# 15.8. Schema Inference

Canonical documentation for [15.8. Schema Inference](2. Data Engineering & Pipelines/15. External Tables/15.8. Schema Inference.md). This document defines concepts, terminology, and standard usage.

## Purpose
Schema Inference addresses the challenge of interacting with unstructured or semi-structured data where an explicit metadata definition (schema) is absent, inaccessible, or incomplete. In modern data processing, data is often produced at high velocity or by disparate systems that do not enforce a rigid contract at the point of origin.

The primary purpose of Schema Inference is to programmatically derive a structured representation—including field names, data types, nesting levels, and nullability—by analyzing the physical data itself. This allows downstream systems to perform typed operations, optimizations, and validations without manual schema declaration.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the logic and mechanics of inference rather than specific software syntax.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * Mechanisms for structural discovery in semi-structured formats (e.g., JSON, XML, Parquet).
> * Type resolution logic and type promotion hierarchies.
> * Sampling strategies and their impact on inference accuracy.
> * Handling of nested structures and arrays.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations or API-specific flags.
> * Performance tuning for specific hardware architectures.
> * Data cleaning or transformation (ETL) processes that occur after the schema is established.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Schema Inference | The process of automatically determining the data structure and types from a representative sample of data. |
| Type Promotion | The logic used to resolve conflicts between different types by selecting the most inclusive common type (e.g., promoting Integer to Float). |
| Structural Discovery | The identification of keys, fields, and nesting levels within a dataset. |
| Sampling Ratio | The percentage of the total dataset analyzed to determine the schema. |
| Nullability | The property of a field indicating whether it can contain missing or undefined values. |
| Heterogeneous Data | A dataset where the same field may contain different data types across different records. |

## Core Concepts
Explain the fundamental ideas.

### Structural Discovery
Before types can be assigned, the inference engine must map the topology of the data. This involves identifying all unique keys in a key-value set or all columns in a tabular format. In hierarchical data (like JSON), this includes flattening or maintaining parent-child relationships.

### Type Resolution Hierarchy
When an inference engine encounters a value, it attempts to match it against a hierarchy of types, usually moving from the most specific to the most general. For example:
1. Boolean
2. Integer
3. Long/BigInt
4. Float/Double
5. String (The "catch-all" type)

### Sampling vs. Full Scan
Inference can be performed on the entire dataset (Full Scan) or a subset (Sampling). 
* **Full Scan:** Guarantees 100% accuracy but is computationally expensive for large datasets.
* **Sampling:** Provides high performance but risks missing "long-tail" fields or rare data types that appear late in the stream.

> [!TIP]
> Think of Schema Inference like a paleontologist reconstructing a skeleton. By examining a few key bones (data points), the scientist can infer the structure of the entire organism, though more bones lead to a more accurate reconstruction.

## Standard Model
The standard model for Schema Inference follows a linear pipeline:

1.  **Data Acquisition:** Accessing a raw byte stream or file.
2.  **Tokenization:** Breaking the data into discrete records and fields based on delimiters or format specifications.
3.  **Type Detection:** Evaluating each field against a set of primitive and complex type rules.
4.  **Conflict Resolution:** If a field "ID" appears as an Integer in record 1 and a String in record 2, the model applies a promotion rule (usually promoting both to String).
5.  **Schema Synthesis:** Consolidating the findings into a formal metadata object (e.g., a DDL statement or a JSON schema).

## Common Patterns
*   **First-Row Inference:** The system assumes the first record is representative of the entire dataset. This is extremely fast but highly fragile.
*   **Multi-Pass Inference:** The system performs one pass to identify all possible keys and a second pass to resolve the types of those keys.
*   **Schema Merging:** In distributed systems, schemas are inferred for individual partitions or files and then merged into a single global schema that represents the union of all discovered fields.

## Anti-Patterns
*   **Inference in Production Pipelines:** Relying on dynamic inference for mission-critical production jobs. This can lead to "Schema Drift," where a single malformed record changes the schema and breaks downstream consumers.
*   **Ignoring Nulls during Sampling:** If the sampling process only encounters null values for a specific field, it may incorrectly infer the type as "Void" or "String," causing failures when actual data eventually appears.
*   **Over-Promotion:** Automatically promoting all ambiguous types to "String" or "Blob" to avoid errors, which sacrifices the benefits of type safety and storage optimization.

> [!CAUTION]
> Avoid circular dependencies where the inference logic depends on a partial schema that is itself being updated by the inference process. This can lead to non-deterministic schema states.

## Edge Cases
*   **Empty Datasets:** If a file is empty, the inference engine has no data to analyze. Standard behavior should be to return an error or a predefined "Empty Schema" rather than guessing.
*   **Deeply Nested Arrays:** Inference engines often have a "depth limit." Data nested beyond this limit may be truncated or treated as a raw string/map.
*   **Homogeneous vs. Heterogeneous Arrays:** An array containing `[1, "two", 3.0]` presents a conflict. The engine must decide whether to treat the array as a list of Strings or a list of Variants/Any types.
*   **Date/Timestamp Ambiguity:** A value like `01/02/03` could be interpreted as `YY/MM/DD`, `DD/MM/YY`, or `MM/DD/YY`. Without locale context, inference is purely speculative.

## Related Topics
*   **Schema Evolution:** How a schema changes over time after the initial inference.
*   **Schema Registry:** A centralized service for storing and retrieving schemas to avoid repeated inference.
*   **Strong Typing vs. Weak Typing:** The underlying philosophy of how data types are enforced.
*   **Serialization/Deserialization (SerDe):** The process of converting data to and from the inferred format.

## Change Log

| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-17 | Initial AI-generated canonical documentation |