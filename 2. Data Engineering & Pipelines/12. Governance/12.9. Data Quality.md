# 12.9. Data Quality

Canonical documentation for [12.9. Data Quality](2. Data Engineering & Pipelines/12. Governance/12.9. Data Quality.md). This document defines concepts, terminology, and standard usage.

## Purpose
Data Quality (DQ) exists to ensure that data is fit for its intended purpose in operations, decision-making, and planning. It addresses the inherent risk that inaccurate, incomplete, or inconsistent data leads to flawed insights, operational inefficiencies, and regulatory non-compliance. By establishing a framework for measuring and maintaining data integrity, organizations can treat data as a reliable strategic asset rather than a liability.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the universal principles of data excellence regardless of the underlying storage or processing technology.

## Scope
The scope of Data Quality encompasses the entire data lifecycle, from ingestion and transformation to storage and consumption.

> [!IMPORTANT]
> **In scope:**
> * The dimensions of data quality (Accuracy, Completeness, etc.).
> * Data profiling and assessment methodologies.
> * Remediation strategies and root cause analysis.
> * Continuous monitoring and reporting frameworks.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations or software-specific syntax.
> * Physical database tuning (unless directly impacting data integrity).
> * General data security (covered under Data Protection).

## Definitions
| Term | Definition |
|------|------------|
| Data Quality Dimension | A measurable characteristic of data used to evaluate its quality. |
| Data Profiling | The process of examining data from an existing source and summarizing information about that data. |
| Remediation | The act of correcting data errors or improving processes to prevent future errors. |
| Data Quality Rule | A formal requirement that data must meet to be considered high quality. |
| Metadata | Data that provides information about other data, essential for interpreting quality context. |
| Truth Source | The system of record designated as the authoritative provider of a specific data element. |

## Core Concepts
The foundation of Data Quality rests on the ability to quantify "fitness for use" through standardized dimensions.

### The Dimensions of Data Quality
1.  **Accuracy:** The degree to which data correctly describes the "real world" object or event being described.
2.  **Completeness:** The proportion of data stored against the potential of 100%.
3.  **Consistency:** The absence of difference between data items representing the same entity across different datasets.
4.  **Timeliness:** The degree to which data is representatively up-to-date and available when needed.
5.  **Validity:** The extent to which data conforms to the syntax (format, type, range) of its definition.
6.  **Uniqueness:** The assurance that no entity is recorded more than once within a dataset.

> [!TIP]
> Think of Data Quality like a supply chain: if the raw materials (data inputs) are contaminated, the final product (business intelligence) will be defective, regardless of how sophisticated the manufacturing (analytics) process is.

## Standard Model
The standard model for Data Quality is a cyclical process known as the **Data Quality Management (DQM) Lifecycle**.

1.  **Define:** Identify critical data elements (CDEs) and establish business rules and quality thresholds.
2.  **Measure:** Perform data profiling and execute quality rules to establish a baseline.
3.  **Analyze:** Identify the gap between current state and requirements; perform root cause analysis on defects.
4.  **Improve:** Implement remediation plans, which may include data cleansing or upstream process changes.
5.  **Control:** Establish ongoing monitoring and alerting to ensure quality does not degrade over time.

## Common Patterns
*   **Shift-Left Validation:** Implementing data quality checks as close to the source of entry as possible to prevent "garbage in."
*   **Data Quality Scorecards:** Aggregating individual rule results into high-level metrics for executive visibility.
*   **Automated Profiling:** Using statistical analysis to automatically discover patterns, distributions, and anomalies in new datasets.
*   **Circuit Breakers:** Programmatic checks in data pipelines that stop processing if data quality falls below a critical threshold to prevent downstream contamination.

## Anti-Patterns
*   **Downstream Fixing:** Correcting data in a data warehouse or reporting layer without fixing the source system, leading to recurring errors.
*   **IT-Only Ownership:** Treating data quality as a purely technical problem rather than a business responsibility.
*   **Boiling the Ocean:** Attempting to apply rigorous quality rules to all data simultaneously rather than prioritizing Critical Data Elements.
*   **Silent Failures:** Allowing data pipelines to complete successfully even when the data produced is logically invalid.

> [!CAUTION]
> Avoid "Siloed Remediation" where data is cleaned for one specific report but remains broken in the central repository, creating "multiple versions of the truth."

## Edge Cases
*   **Subjective Data:** Quality assessment for unstructured data (e.g., sentiment analysis or free-text notes) where "accuracy" is a matter of interpretation.
*   **Late-Arriving Data:** Scenarios where data is accurate but arrives after a decision-making window has closed, rendering it "low quality" for that specific use case.
*   **Decoupled Schema Evolution:** When upstream source systems change their data structures without notifying downstream consumers, causing valid data to fail "validity" checks.
*   **Probabilistic Matching:** In entity resolution, data quality is often a percentage of confidence rather than a binary "correct/incorrect" state.

## Related Topics
*   **Data Governance:** The overarching framework of roles, policies, and standards that Data Quality supports.
*   **Master Data Management (MDM):** The discipline of creating a single, accurate view of key business entities.
*   **Data Lineage:** The map of data's journey, essential for performing root cause analysis of quality issues.
*   **Metadata Management:** Providing the context necessary to define what "quality" looks like for a specific attribute.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-17 | Initial AI-generated canonical documentation |