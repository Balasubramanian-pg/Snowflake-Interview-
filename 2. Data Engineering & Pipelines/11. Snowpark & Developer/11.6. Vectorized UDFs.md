# 11.6. Vectorized UDFs

Canonical documentation for [11.6. Vectorized UDFs](2. Data Engineering & Pipelines/11. Snowpark & Developer/11.6. Vectorized UDFs.md). This document defines concepts, terminology, and standard usage.

## Purpose
The purpose of Vectorized User-Defined Functions (UDFs) is to bridge the performance gap between native system functions and custom user logic in data processing environments. Traditional UDFs typically operate on a row-by-row basis (the "Scalar" model), which introduces significant overhead due to frequent function calls, context switching between the execution engine and the user runtime, and inefficient memory access patterns.

Vectorized UDFs address these inefficiencies by processing batches of data—represented as vectors or arrays—in a single invocation. This approach leverages modern CPU architectures and minimizes the relative cost of the orchestration layer.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural patterns rather than specific programming language syntax.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * Data transfer mechanisms between execution engines and UDF runtimes.
> * Memory layout requirements for vectorized execution.
> * Performance characteristics and amortized overhead models.
> * Logical mapping of input batches to output batches.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Apache Spark Pandas UDFs, Snowflake Python UDFs).
> * Syntax-specific tutorials for individual programming languages.
> * Hardware-level SIMD instruction set architecture (ISA) specifications.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Vector | A contiguous sequence of data elements of the same type, typically representing a segment of a column. |
| Batch | A collection of vectors representing multiple columns for a fixed number of rows. |
| Scalar UDF | A function applied to one row at a time, returning one result per invocation. |
| SerDe | Serialization and Deserialization; the process of converting data structures into a format suitable for transfer between environments. |
| Amortized Overhead | The reduction of fixed per-call costs by spreading them across a large number of data points in a single call. |
| SIMD | Single Instruction, Multiple Data; a hardware capability allowing one instruction to process multiple data points simultaneously. |

## Core Concepts
The fundamental idea behind Vectorized UDFs is the transition from "tuple-at-a-time" processing to "block-at-a-time" processing.

**1. Amortization of Invocation Costs**
In a scalar UDF, the overhead of calling the function (setting up the stack, checking types, handling nulls) is incurred for every single row. In a vectorized UDF, this overhead is incurred once per batch (e.g., once per 1,024 rows), making the per-row overhead negligible.

**2. Columnar Memory Layout**
Vectorized UDFs rely on columnar memory formats. By storing data for a single column contiguously in memory, the system maximizes CPU cache hits and enables the use of SIMD instructions.

**3. Zero-Copy Transfers**
High-performance vectorized implementations often strive for "zero-copy" data sharing, where the execution engine and the UDF runtime point to the same memory address for the data batch, avoiding expensive data duplication.

> [!TIP]
> Think of a Scalar UDF as a toll booth where every car must stop, pay, and wait for the gate to open. Think of a Vectorized UDF as a ferry that waits for 100 cars to board, crosses the river once, and unloads them all at the destination. The "per-car" transit time is significantly lower on the ferry.

## Standard Model
The standard model for Vectorized UDF execution follows a structured pipeline:

1.  **Batch Formation:** The execution engine groups rows into a memory-resident batch.
2.  **Context Transition:** The engine passes a reference (pointer) or a serialized representation of the batch to the UDF runtime.
3.  **Vectorized Execution:** The user-defined logic iterates over the arrays using optimized libraries (e.g., NumPy, Arrow, or native loops) that operate on the entire vector.
4.  **Result Alignment:** The UDF returns a result vector of the same cardinality as the input batch (for Map operations) or a reduced set (for Aggregations).
5.  **Integration:** The engine integrates the returned vector back into the primary data stream.

## Common Patterns
*   **Element-wise Transformation (Map):** Taking one or more input vectors and producing an output vector of the same length (e.g., `VectorC = VectorA + VectorB`).
*   **Vectorized Aggregation:** Taking an input vector and returning a single scalar value or a reduced vector (e.g., `Sum(VectorA)`).
*   **Predicate Filtering:** Evaluating a condition across a vector to produce a boolean mask, which is then used to filter the batch.

## Anti-Patterns
*   **Row-Looping inside the UDF:** Writing a vectorized UDF but using a standard `for-each` loop in a high-level language (like Python) to process the elements. This negates the performance benefits of vectorization.
*   **Heavy I/O Operations:** Performing database lookups or API calls inside a vectorized function. This introduces latencies that the vectorized model is designed to avoid.
*   **Frequent Small Batches:** Setting the batch size too low (e.g., 5 rows), which causes the overhead of the vectorized machinery to outweigh the processing gains.

> [!CAUTION]
> Avoid mixing vectorized logic with stateful row-level dependencies that require cross-row communication unless the framework explicitly supports windowed vectorized operations.

## Edge Cases
*   **Null Handling:** Vectors must account for null values, often through a secondary "null bitmap" or "mask." If the UDF logic does not explicitly handle the mask, it may produce incorrect results or crash.
*   **Type Coercion:** When a batch contains mixed types or values that exceed the precision of the vectorized container (e.g., an integer overflow in a fixed-width array).
*   **Sparse Data:** Vectorization is highly efficient for dense data but may lead to wasted memory and processing cycles when dealing with extremely sparse vectors if the implementation does not support compressed formats.
*   **Batch Size Mismatch:** Scenarios where the final batch of a dataset is smaller than the standard batch size, requiring the UDF to handle variable-length inputs gracefully.

## Related Topics
*   **Columnar Storage:** The underlying data format (e.g., Parquet, ORC) that enables efficient vectorization.
*   **SIMD Optimization:** The hardware-level acceleration that vectorized code often targets.
*   **Apache Arrow:** A cross-language development platform for in-memory data that specifies a standardized columnar memory format.
*   **Query Compilation:** An alternative to vectorization where UDFs are compiled into machine code alongside the engine's native code.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-02-17 | Initial AI-generated canonical documentation |